{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Enpei - PS4",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu"
      },
      "source": [
        "#EECS 442 PS4: Backpropagation\n",
        "\n",
        "__Please provide the following information__\n",
        "(e.g. Andrew Owens, ahowens):\n",
        "\n",
        "Enpei Zhao, zepxyz\n",
        "\n",
        "__Important__: after you download the .ipynb file, please name it as __\\<your_uniquename\\>_\\<your_umid\\>.ipynb__ before you submit it to canvas. Example: adam_01101100.ipynb.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHumIO-xt57H"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC"
      },
      "source": [
        "# Problem 4.2 Softmax Classifier with Two Layer Neural Network\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ljfgMv9PHx"
      },
      "source": [
        "def fc_forward(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input X has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - X: A numpy array containing input data, of shape (N, Din)\n",
        "    - W: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (X, W, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    out = np.dot(X, W) + b\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (X, W, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - X: Input data, of shape (N, Din)\n",
        "      - W: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dX: Gradient with respect to X, of shape (N, Din)\n",
        "    - dW: Gradient with respect to W, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    X, W, b = cache\n",
        "    dX, dW, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    dX = np.dot(dout, np.transpose(W))\n",
        "    dW = np.dot(np.transpose(X), dout)\n",
        "    db = np.dot(np.transpose(dout), np.ones(dout.shape[0]))\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dX, dW, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    out[x < 0] = 0\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout, cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    #dx[x <= 0] = 0\n",
        "    dx = np.multiply((x > 0).astype(int), dout) \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(X, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - X: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for X[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dX: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dX = None, None                                         #\n",
        "    dX = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    dX /= np.sum(dX, axis=1, keepdims=True)\n",
        "    loss = -np.sum(np.log(dX[np.arange(X.shape[0]), y])) / X.shape[0]\n",
        "    dX[np.arange(X.shape[0]), y] -= 1\n",
        "    dX /= X.shape[0]\n",
        "\n",
        "    return loss, dX"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz"
      },
      "source": [
        "# 4.2 (b) Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvxbx9UpxVL"
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        self.params['W1'] = np.random.randn(input_dim, hidden_dim) * weight_scale \n",
        "        self.params['b1'] = np.zeros(hidden_dim)\n",
        "        self.params['W2'] = np.random.randn(hidden_dim, num_classes) * weight_scale \n",
        "        self.params['b2'] = np.zeros(num_classes)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        W1 = self.params['W1']\n",
        "        b1 = self.params['b1']\n",
        "        W2 = self.params['W2']\n",
        "        b2 = self.params['b2']\n",
        "\n",
        "        # first layer\n",
        "        out1, cache1 = fc_forward(X, W1, b1)\n",
        "        # relu\n",
        "        out1, cache_relu = relu_forward(out1)\n",
        "        # second layer\n",
        "        scores, cache_scores = fc_forward(out1, W2, b2)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "        loss, dscores = softmax_loss(scores, y)\n",
        "        # second layer\n",
        "        dx1, dW2, db2 = fc_backward(dscores, cache_scores)\n",
        "        # relu\n",
        "        dx1 = relu_backward(dx1, cache_relu)\n",
        "        # first layer\n",
        "        dx, dW1, db1 = fc_backward(dx1, cache1)\n",
        "\n",
        "        grads['W1'] = dW1\n",
        "        grads['b1'] = db1\n",
        "        grads['W2'] = dW2\n",
        "        grads['b2'] = db2\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "outputId": "0fffa364-e6cd-424d-fcff-5a3ce1d80a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def testNetwork(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "def SGD(W,dW, learning_rate=1e-3):\n",
        "    \"\"\" Apply a gradient descent step on weight W \n",
        "    Inputs:\n",
        "        W : Weight matrix\n",
        "        dW : gradient of weight, same shape as W\n",
        "        learning_rate : Learning rate. Defaults to 1e-3.\n",
        "    Returns:\n",
        "        new_W: Updated weight matrix\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n",
        "    new_W = W - learning_rate * dW\n",
        "\n",
        "    return new_W\n",
        "\n",
        "def trainNetwork(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    - optimizer: Choice of using either 'SGD' or 'SGD_Momentum' for updating weights; default is SGD.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    optimizer = kwargs.pop('optimizer', 'SGD')\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "    #Initialize velocity dictionary if optimizer is SGD_Momentum\n",
        "    if optimizer == 'SGD_Momentum':\n",
        "      velocity_dict = {p:np.zeros(w.shape) for p,w in model.params.items()}\n",
        "      \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        if optimizer == 'SGD':\n",
        "          for p, w in model.params.items():\n",
        "              model.params[p] = SGD(w,grads[p], learning_rate=learning_rate)\n",
        "\n",
        "        elif optimizer == 'SGD_Momentum':\n",
        "          for p, w in model.params.items():\n",
        "              model.params[p], velocity_dict[p] = SGD_Momentum(w, grads[p], velocity_dict[p], beta=0.5, learning_rate=learning_rate)\n",
        "        else:\n",
        "          raise NotImplementedError\n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = testNetwork(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = testNetwork(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters for SGD                               #\n",
        "#######################################################################\n",
        "\n",
        "# initialize model\n",
        "model_SGD = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n",
        "\n",
        "# start training using SGD\n",
        "model_SGD, train_acc_history_SGD, val_acc_history_SGD = trainNetwork(\n",
        "    model_SGD, train_data, learning_rate = 0.01,\n",
        "    lr_decay=1, num_epochs=10, \n",
        "    batch_size=100, print_every=1000, optimizer = 'SGD')\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 4000) loss: 2.300300\n",
            "(Epoch 0 / 10) train acc: 0.149000; val_acc: 0.112000\n",
            "(Epoch 1 / 10) train acc: 0.374000; val_acc: 0.364100\n",
            "(Epoch 2 / 10) train acc: 0.431000; val_acc: 0.404800\n",
            "(Iteration 1001 / 4000) loss: 1.704409\n",
            "(Epoch 3 / 10) train acc: 0.467000; val_acc: 0.429700\n",
            "(Epoch 4 / 10) train acc: 0.485000; val_acc: 0.444800\n",
            "(Epoch 5 / 10) train acc: 0.498000; val_acc: 0.462200\n",
            "(Iteration 2001 / 4000) loss: 1.380786\n",
            "(Epoch 6 / 10) train acc: 0.549000; val_acc: 0.471300\n",
            "(Epoch 7 / 10) train acc: 0.535000; val_acc: 0.483300\n",
            "(Iteration 3001 / 4000) loss: 1.345362\n",
            "(Epoch 8 / 10) train acc: 0.554000; val_acc: 0.486700\n",
            "(Epoch 9 / 10) train acc: 0.543000; val_acc: 0.491200\n",
            "(Epoch 10 / 10) train acc: 0.576000; val_acc: 0.503900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ilTVXIw_7q"
      },
      "source": [
        "# 4.2(d) Training with SGD_Momentum\n",
        "\n",
        "The model above was trained using SGD. Now implement the SGD_Momentum function to train the model using SGD with momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54jGVPZOXtV6",
        "outputId": "e334c6ff-0d18-4b48-b867-6371e13d7490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "def SGD_Momentum(W, dW, velocity, beta=0.5, learning_rate=1e-3):\n",
        "    \"\"\" Apply a gradient descent with momentum update on weight W\n",
        "    Inputs:\n",
        "        W : Weight matrix\n",
        "        dW : gradient of weight, same shape as W\n",
        "        velocity : velocity matrix, same shape as W\n",
        "        beta : scalar value in range [0,1] weighting the velocity matrix. Setting it to 0 should make SGD_Momentum same as SGD. \n",
        "               Defaults to 0.5.\n",
        "        learning_rate : Learning rate. Defaults to 1e-3.\n",
        "    Returns:\n",
        "        new_W: Updated weight matrix\n",
        "        new_velocity: Updated velocity matrix\n",
        "    \"\"\"\n",
        "    # ===== your code here! =====\n",
        "    # TODO:\n",
        "    # Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n",
        "    # 1. Calculate the new velocity by using the velocity of last iteration (input velocity) and gradient\n",
        "    # 2. Update the weights using the new_velocity\n",
        "    new_velocity = dW + velocity * beta\n",
        "    new_W = W - learning_rate * new_velocity\n",
        "    # ==== end of code ====\n",
        "    return new_W, new_velocity\n",
        "\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters for SGD_Momentum   \n",
        "# Your hyperparameters should be identical to what you used for SGD (without momentum)#\n",
        "#######################################################################\n",
        "\n",
        "# initialize model\n",
        "model_SGD_Momentum = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n",
        "\n",
        "# start training \n",
        "#Using SGD_Momentum as optimizer for trainning for training\n",
        "model_SGD_Momentum, train_acc_history_SGD_Momentum, val_acc_history_SGD_Momentum = trainNetwork(\n",
        "    model_SGD_Momentum, train_data, learning_rate = 0.01,\n",
        "    lr_decay=1, num_epochs=10, \n",
        "    batch_size=100, print_every=1000, optimizer = 'SGD_Momentum')\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 4000) loss: 2.293590\n",
            "(Epoch 0 / 10) train acc: 0.131000; val_acc: 0.114100\n",
            "(Epoch 1 / 10) train acc: 0.421000; val_acc: 0.401500\n",
            "(Epoch 2 / 10) train acc: 0.474000; val_acc: 0.446700\n",
            "(Iteration 1001 / 4000) loss: 1.311558\n",
            "(Epoch 3 / 10) train acc: 0.499000; val_acc: 0.463100\n",
            "(Epoch 4 / 10) train acc: 0.532000; val_acc: 0.479100\n",
            "(Epoch 5 / 10) train acc: 0.607000; val_acc: 0.495000\n",
            "(Iteration 2001 / 4000) loss: 1.257793\n",
            "(Epoch 6 / 10) train acc: 0.597000; val_acc: 0.500000\n",
            "(Epoch 7 / 10) train acc: 0.601000; val_acc: 0.507400\n",
            "(Iteration 3001 / 4000) loss: 1.248624\n",
            "(Epoch 8 / 10) train acc: 0.621000; val_acc: 0.507600\n",
            "(Epoch 9 / 10) train acc: 0.643000; val_acc: 0.512200\n",
            "(Epoch 10 / 10) train acc: 0.660000; val_acc: 0.514900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa"
      },
      "source": [
        "# 4.2(e) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy of model_SGD and model_SGD_Momentum on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCq8pBhu6dz",
        "outputId": "8d23760f-4a5b-487e-fa0f-41f828e7c0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# report test accuracy\n",
        "acc = testNetwork(model_SGD, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy of model_SGD: {}\".format(acc))\n",
        "# report test accuracy\n",
        "acc = testNetwork(model_SGD_Momentum, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy of model_SGD_Momentum: {}\".format(acc))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy of model_SGD: 0.5003\n",
            "Test accuracy of model_SGD_Momentum: 0.5064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N"
      },
      "source": [
        "# 4.2(f) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot, using SGD and SGD_Momentum as optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjtnbya9S7g",
        "outputId": "c394be7c-dedb-48c0-b3df-5d9f0c0eaca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "#######################################################################\n",
        "# Your Code here\n",
        "#######################################################################\n",
        "plt.plot(train_acc_history_SGD)\n",
        "plt.plot(val_acc_history_SGD)\n",
        "plt.plot(train_acc_history_SGD_Momentum)\n",
        "plt.plot(val_acc_history_SGD_Momentum)\n",
        "plt.legend(['train', 'val', 'train_SGD', 'val_SGD'], loc='lower right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIhQyFlw4HPL"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}