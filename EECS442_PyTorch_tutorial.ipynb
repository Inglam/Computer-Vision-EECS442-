{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EECS442_PyTorch_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RIoujYkcPa3p",
        "YWZ3ApwNhFAS",
        "4K8N5ZiMkHNh",
        "mUTtbOHpPTTY",
        "Geb689ATQA1U",
        "liwMHyYfxqS8",
        "gzAoAQfq_NWw",
        "wVEAVv-6ooaE",
        "5Srt97Okfc8y",
        "W2TPaZ3mjmRO",
        "w9BUOhaojo_o",
        "6EQjX3_Xl_Cs"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inglam/Computer-Vision-EECS442-/blob/master/EECS442_PyTorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3jwhXn4hGU-"
      },
      "source": [
        "# EECS 442 Computer Vision \n",
        "# PyTorch Tutorial\n",
        "\n",
        "2020-10-02\n",
        "\n",
        "# Introduction\n",
        "\n",
        "PyTorch will be used throughout the remaining of the semseter. This tutorial first covers the basics of PyTorch. At the end, we will train a neural network for classifying handwritten digits together. Some of the material draws from [EECS498/598](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/), [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html#deep-learning-with-pytorch-a-60-minute-blitz), [Python Data Science Handbook](https://learning.oreilly.com/library/view/python-data-science/9781491912126/), and [EECS 504 PyTorch Tutorial](https://drive.google.com/open?id=1lRdeX1rO4KShTM-WbM2qyHXjcur3Iao5).\n",
        "\n",
        "You can learn more by reading the official PyTorch book written by the PyTorch development team. [link](https://pytorch.org/deep-learning-with-pytorch)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G_A-Q99LceL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "140a4b62-b558-4f5d-b0ac-d4df8c798e65"
      },
      "source": [
        "!pip install torchviz\n",
        "!pip install torchsummary\n",
        "import torch\n",
        "import torchviz\n",
        "from torchsummary import summary\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.6.0+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3520 sha256=9919ea72a6fb7d3469a0bae3deb704ed7c2d21c855aa4909d445beccb5cb9f12\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPV_fbrphx-m"
      },
      "source": [
        "# Tensors\n",
        "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
        "\n",
        "Essentially, PyTorch is NumPy that can run on GPUs and allow automatic differentiation for building and training neural networks models. PyTorch tensors are similar to NumPy's ndarrays. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIoujYkcPa3p"
      },
      "source": [
        "## Basic Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylcoceZ5NFBD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afc0ed92-4f61-4770-b9a9-485a63267535"
      },
      "source": [
        "# Construct a tensor from data\n",
        "x = torch.tensor([5.5, 3.1])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.5000, 3.1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO6tnAY7LQqj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "19638b6c-9a5c-4058-de48-b4fea12cc4ad"
      },
      "source": [
        "# construct a 5x3 matrix, uninitialized\n",
        "x = torch.empty(5, 3)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.1972e-35, 0.0000e+00, 3.3631e-44],\n",
            "        [0.0000e+00,        nan, 0.0000e+00],\n",
            "        [1.1578e+27, 1.1362e+30, 7.1547e+22],\n",
            "        [4.5828e+30, 1.2121e+04, 7.1846e+22],\n",
            "        [9.2198e-39, 7.0374e+22, 5.0948e-14]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyrVFgEWLeq9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7e69b002-010a-4a4c-be86-637007f181f6"
      },
      "source": [
        "# construct a randomly initialized matrix\n",
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8963, 0.3735, 0.4606],\n",
            "        [0.6429, 0.5867, 0.9006],\n",
            "        [0.4683, 0.0620, 0.3558],\n",
            "        [0.4663, 0.2026, 0.4600],\n",
            "        [0.2671, 0.8559, 0.3861]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFAeCTwRLna9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5329e5b2-749d-4b71-e24d-08c8ec1a8d15"
      },
      "source": [
        "# construct a matrix filled with zeros and of dtype long\n",
        "x = torch.zeros(5, 3, dtype=torch.long)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chuBbGkuNLXi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3099b7d0-5b4c-4e10-a3ee-d6596ab59b8e"
      },
      "source": [
        "# you can get the size of the tensor\n",
        "print(x.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SYdZSSUNTRm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "75b80203-0cb0-4c34-c782-3950519b286c"
      },
      "source": [
        "# similar to numpy.ndarray, torch.tensor supports many basic arithmetic operations\n",
        "x, y = torch.rand(5, 3), torch.rand(5, 3)\n",
        "print(x, y, x + y)\n",
        "\n",
        "# in-place operation\n",
        "y.add_(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8412, 0.2671, 0.7117],\n",
            "        [0.4817, 0.5758, 0.6747],\n",
            "        [0.4581, 0.5103, 0.8931],\n",
            "        [0.0511, 0.4599, 0.6924],\n",
            "        [0.5441, 0.7341, 0.1286]]) tensor([[0.3077, 0.2417, 0.8462],\n",
            "        [0.0786, 0.7811, 0.7223],\n",
            "        [0.7699, 0.6730, 0.3770],\n",
            "        [0.4401, 0.7300, 0.9534],\n",
            "        [0.9963, 0.7539, 0.1993]]) tensor([[1.1489, 0.5088, 1.5579],\n",
            "        [0.5603, 1.3569, 1.3970],\n",
            "        [1.2280, 1.1834, 1.2701],\n",
            "        [0.4912, 1.1899, 1.6458],\n",
            "        [1.5404, 1.4880, 0.3279]])\n",
            "tensor([[1.1489, 0.5088, 1.5579],\n",
            "        [0.5603, 1.3569, 1.3970],\n",
            "        [1.2280, 1.1834, 1.2701],\n",
            "        [0.4912, 1.1899, 1.6458],\n",
            "        [1.5404, 1.4880, 0.3279]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlvBgS0SPEeg"
      },
      "source": [
        "Resizing is a useful operation for torch.tensors. when training machine learning models, somes you want to change shape your input tensor and `torch.view()` method will come in handy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGq9MX97OLNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2861323e-5afb-46a9-c506-082a0cfabadb"
      },
      "source": [
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)\n",
        "print(x.size(), y.size(), z.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMEyAAC-O-ur"
      },
      "source": [
        "To access the Python number sotred in a one element tensor (such as the loss value), you can use .item() to get the value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiZmCrNGOrtF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52eac08b-23d1-4a27-8db1-fe396dcb262d"
      },
      "source": [
        "x = torch.randn(1)\n",
        "print(x, x.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.8008]) 0.8008484244346619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWZ3ApwNhFAS"
      },
      "source": [
        "## Tensor Indexing\n",
        "\n",
        "Similar to Python lists and numpy arrays, PyTorch tensors can be **sliced** using the syntax `start:stop` or `start:stop:step`. The `stop` index is always non-inclusive: it is the first element not to be included in the slice.\n",
        "\n",
        "Start and stop indices can be negative, in which case they count backward from the end of the tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWBoWJJWhz6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43ba516-46ca-4613-a264-7acfa3382e86"
      },
      "source": [
        "a = torch.tensor([0, 11, 22, 33, 44, 55, 66])\n",
        "print(0, a)        # (0) Original tensor\n",
        "print(1, a[2:5])   # (1) Elements between index 2 and 5\n",
        "print(2, a[2:])    # (2) Elements after index 2\n",
        "print(3, a[:5])    # (3) Elements before index 5\n",
        "print(4, a[:])     # (4) All elements\n",
        "print(5, a[1:5:2]) # (5) Every second element between indices 1 and 5\n",
        "print(6, a[:-1])   # (6) All but the last element\n",
        "print(7, a[-4::2]) # (7) Every second element, starting from the fourth-last"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor([ 0, 11, 22, 33, 44, 55, 66])\n",
            "1 tensor([22, 33, 44])\n",
            "2 tensor([22, 33, 44, 55, 66])\n",
            "3 tensor([ 0, 11, 22, 33, 44])\n",
            "4 tensor([ 0, 11, 22, 33, 44, 55, 66])\n",
            "5 tensor([11, 33])\n",
            "6 tensor([ 0, 11, 22, 33, 44, 55])\n",
            "7 tensor([33, 55])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qgtIcp_h1h7"
      },
      "source": [
        "For multidimensional tensors, you can provide a slice or integer for each dimension of the tensor in order to extract different types of subtensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63m_W_wSh8OF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751c9480-e0e0-40f6-bc88-cc284d9c3067"
      },
      "source": [
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "# [[ 1  2  3  4]\n",
        "#  [ 5  6  7  8]\n",
        "#  [ 9 10 11 12]]\n",
        "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "print('shape: ', a.shape)\n",
        "\n",
        "# Get row 1, and all columns. \n",
        "print('\\nSingle row:')\n",
        "print(a[1, :])\n",
        "print(a[1])  # Gives the same result; we can omit : for trailing dimensions\n",
        "print('shape: ', a[1].size(), a[1, :].shape)\n",
        "\n",
        "print('\\nSingle column:')\n",
        "print(a[:, 1])\n",
        "print('shape: ', a[:, 1].shape)\n",
        "\n",
        "# Get the first two rows and the last three columns\n",
        "print('\\nFirst two rows, last three columns:')\n",
        "print(a[:2, -3:])\n",
        "print('shape: ', a[:2, -3:].shape)\n",
        "\n",
        "# Get every other row, and columns at index 1 and 2\n",
        "print('\\nEvery other row, middle columns:')\n",
        "print(a[::2, 1:3])\n",
        "print('shape: ', a[::2, 1:3].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11, 12]])\n",
            "shape:  torch.Size([3, 4])\n",
            "\n",
            "Single row:\n",
            "tensor([5, 6, 7, 8])\n",
            "tensor([5, 6, 7, 8])\n",
            "shape:  torch.Size([4]) torch.Size([4])\n",
            "\n",
            "Single column:\n",
            "tensor([ 2,  6, 10])\n",
            "shape:  torch.Size([3])\n",
            "\n",
            "First two rows, last three columns:\n",
            "tensor([[2, 3, 4],\n",
            "        [6, 7, 8]])\n",
            "shape:  torch.Size([2, 3])\n",
            "\n",
            "Every other row, middle columns:\n",
            "tensor([[ 2,  3],\n",
            "        [10, 11]])\n",
            "shape:  torch.Size([2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STVqJBaUiMCQ"
      },
      "source": [
        "There are two common ways to access a single row or column of a tensor: using an integer will reduce the rank by one, and using a length-one slice will keep the same rank. Note that this is different behavior from MATLAB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSMR027niNKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a967124b-f2f1-4ab4-9b76-725764ece61f"
      },
      "source": [
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "print('Original tensor')\n",
        "print(a)\n",
        "\n",
        "row_r1 = a[1, :]    # Rank 1 view of the second row of a  \n",
        "row_r2 = a[1:2, :]  # Rank 2 view of the second row of a\n",
        "print('\\nTwo ways of accessing a single row:')\n",
        "print(row_r1, row_r1.shape)\n",
        "print(row_r2, row_r2.shape)\n",
        "\n",
        "# We can make the same distinction when accessing columns::\n",
        "col_r1 = a[:, 1]\n",
        "col_r2 = a[:, 1:2]\n",
        "print('\\nTwo ways of accessing a single column:')\n",
        "print(col_r1, col_r1.shape)\n",
        "print(col_r2, col_r2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tensor\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11, 12]])\n",
            "\n",
            "Two ways of accessing a single row:\n",
            "tensor([5, 6, 7, 8]) torch.Size([4])\n",
            "tensor([[5, 6, 7, 8]]) torch.Size([1, 4])\n",
            "\n",
            "Two ways of accessing a single column:\n",
            "tensor([ 2,  6, 10]) torch.Size([3])\n",
            "tensor([[ 2],\n",
            "        [ 6],\n",
            "        [10]]) torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfT8qZOViVe1"
      },
      "source": [
        "Slicing a tensor returns a **view** into the same data, so modifying it will also modify the original tensor. To avoid this, you can use the `clone()` method to make a copy of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZOCphuIiWbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac21a8a3-017b-42c7-b615-125f0df68bd7"
      },
      "source": [
        "# Create a tensor, a slice, and a clone of a slice\n",
        "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "b = a[0, 1:]\n",
        "c = a[0, 1:].clone()\n",
        "print('Before mutating:')\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "\n",
        "a[0, 1] = 20  # a[0, 1] and b[0] point to the same element\n",
        "b[1] = 30     # b[1] and a[0, 2] point to the same element\n",
        "c[2] = 40     # c is a clone, so it has its own data\n",
        "print('\\nAfter mutating:')\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "\n",
        "print(a.storage().data_ptr() == c.storage().data_ptr())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before mutating:\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "tensor([2, 3, 4])\n",
            "tensor([2, 3, 4])\n",
            "\n",
            "After mutating:\n",
            "tensor([[ 1, 20, 30,  4],\n",
            "        [ 5,  6,  7,  8]])\n",
            "tensor([20, 30,  4])\n",
            "tensor([ 2,  3, 40])\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N2hz2heicIQ"
      },
      "source": [
        "Boolean tensor indexing lets you pick out arbitrary elements of a tensor according to a boolean mask. Frequently this type of indexing is used to select or modify the elements of a tensor that satisfy some condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfFnWUQwjRyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d79249-877c-4863-8799-bfbc76cfb589"
      },
      "source": [
        "a = torch.tensor([[1,2], [3, 4], [5, 6]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# Find the elements of a that are bigger than 3. The mask has the same shape as\n",
        "# a, where each element of mask tells whether the corresponding element of a\n",
        "# is greater than three.\n",
        "mask = (a > 3)\n",
        "print('\\nMask tensor:')\n",
        "print(mask)\n",
        "\n",
        "# We can use the mask to construct a rank-1 tensor containing the elements of a\n",
        "# that are selected by the mask\n",
        "print('\\nSelecting elements with the mask:')\n",
        "print(a[mask])\n",
        "\n",
        "# We can also use boolean masks to modify tensors; for example this sets all\n",
        "# elements <= 3 to zero:\n",
        "a[a <= 3] = 0\n",
        "print('\\nAfter modifying with a mask:')\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "\n",
            "Mask tensor:\n",
            "tensor([[False, False],\n",
            "        [False,  True],\n",
            "        [ True,  True]])\n",
            "\n",
            "Selecting elements with the mask:\n",
            "tensor([4, 5, 6])\n",
            "\n",
            "After modifying with a mask:\n",
            "tensor([[0, 0],\n",
            "        [0, 4],\n",
            "        [5, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K8N5ZiMkHNh"
      },
      "source": [
        "## Broadcasting\n",
        "\n",
        "Broadcasting is a powerful mechanism that allows PyTorch to work with arrays of different shapes when performing arithmetic operations. Frequently we have a smaller tensor and a larger tensor, and we want to use the smaller tensor multiple times to perform some operation on the larger tensor.\n",
        "\n",
        "For example, suppose that we want to add a constant vector to each row of a tensor. We could do it like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPDIatqFktZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c925b6-cfcb-48eb-90c4-8c30c74d0b67"
      },
      "source": [
        "# We will add the vector v to each row of the matrix x,\n",
        "# storing the result in the matrix y\n",
        "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
        "v = torch.tensor([1, 0, 1])\n",
        "y = torch.zeros_like(x)   # Create an empty matrix with the same shape as x\n",
        "\n",
        "# Add the vector v to each row of the matrix x with an explicit loop\n",
        "for i in range(4):\n",
        "    y[i, :] = x[i, :] + v\n",
        "\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZSvso2Pkwg3"
      },
      "source": [
        "This works; however when the tensor x is very large, computing an explicit loop in Python could be slow. Note that adding the vector v to each row of the tensor x is equivalent to forming a tensor vv by stacking multiple copies of v vertically, then performing elementwise summation of x and vv. We could implement this approach like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6ZTOsZjkvLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75778f63-c3ac-4ced-a4f9-94ba55d093a2"
      },
      "source": [
        "vv = v.repeat((4, 1))  # Stack 4 copies of v on top of each other\n",
        "print(vv)              # Prints \"[[1 0 1]\n",
        "                       #          [1 0 1]\n",
        "                       #          [1 0 1]\n",
        "                       #          [1 0 1]]\"\n",
        "\n",
        "y = x + vv  # Add x and vv elementwise\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 0, 1],\n",
            "        [1, 0, 1],\n",
            "        [1, 0, 1],\n",
            "        [1, 0, 1]])\n",
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viEJOuDBk6nc"
      },
      "source": [
        "PyTorch broadcasting allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYFRiI-Yk7uF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafda598-ac98-4478-8573-a77087e9e1b9"
      },
      "source": [
        "# We will add the vector v to each row of the matrix x,\n",
        "# storing the result in the matrix y\n",
        "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
        "v = torch.tensor([1, 0, 1])\n",
        "y = x + v  # Add v to each row of x using broadcasting\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyEbcPPelAae"
      },
      "source": [
        "The line y = x + v works even though x has shape (4, 3) and v has shape (3,) due to broadcasting; this line works as if v actually had shape (4, 3), where each row was a copy of v, and the sum was performed elementwise.\n",
        "\n",
        "Broadcasting two tensors together follows these rules:\n",
        "\n",
        "1.   If the tensors do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.\n",
        "2.   The two tensors are said to be *compatible* in a dimension if they have the same size in the dimension, or if one of the tensors has size 1 in that dimension.\n",
        "3.   The tensors can be broadcast together if they are compatible in all dimensions.\n",
        "4.   After broadcasting, each tensor behaves as if it had shape equal to the elementwise maximum of shapes of the two input tensors.\n",
        "5.   In any dimension where one tensor had size 1 and the other tensor had size greater than 1, the first tensor behaves as if it were copied along that dimension\n",
        "\n",
        "If this explanation does not make sense, try reading the explanation from the [documentation](https://pytorch.org/docs/stable/notes/broadcasting.html).\n",
        "\n",
        "Not all functions support broadcasting. You can find functions that does not support broadcasting from the official docs. (e.g. [`torch.mm`](https://pytorch.org/docs/stable/torch.html#torch.mm) does not support broadcasting, but [`torch.matmul`](https://pytorch.org/docs/1.1.0/torch.html#torch.matmul) does)\n",
        "\n",
        "Broadcasting can let us easily implement many different operations. For example we can compute an outer product of vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oErzjaesl2Y7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ac8a46-092d-4f3b-f08d-650c70edaba4"
      },
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x has shape (2, 3)\n",
        "c = torch.tensor([1, 10, 11, 100])        # c has shape (4)\n",
        "print('Here is the matrix:')\n",
        "print(x)\n",
        "print('\\nHere is the vector:')\n",
        "print(c)\n",
        "\n",
        "# We do the following:\n",
        "# 1. Reshape c from (4,) to (4, 1, 1)\n",
        "# 2. x has shape (2, 3). Since they have different ranks, when we multiply the\n",
        "#    two, x behaves as if its shape were (1, 2, 3)\n",
        "# 3. The result of the broadcast multiplication between tensor of shape\n",
        "#    (4, 1, 1) and (1, 2, 3) has shape (4, 2, 3)\n",
        "# 4. The result y has shape (4, 2, 3), and y[i] (shape (2, 3)) is equal to\n",
        "#    c[i] * x\n",
        "y = c.view(-1, 1, 1) * x\n",
        "print('\\nMultiply x by a set of constants:')\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is the matrix:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Here is the vector:\n",
            "tensor([  1,  10,  11, 100])\n",
            "\n",
            "Multiply x by a set of constants:\n",
            "tensor([[[  1,   2,   3],\n",
            "         [  4,   5,   6]],\n",
            "\n",
            "        [[ 10,  20,  30],\n",
            "         [ 40,  50,  60]],\n",
            "\n",
            "        [[ 11,  22,  33],\n",
            "         [ 44,  55,  66]],\n",
            "\n",
            "        [[100, 200, 300],\n",
            "         [400, 500, 600]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUTtbOHpPTTY"
      },
      "source": [
        "## NumPy Bridge\n",
        "\n",
        "We can easily convert a torch.tensor to a numpy.ndarray and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEUS01pZPZwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674e8062-b926-45e1-8583-45518c4a24c6"
      },
      "source": [
        "# torch.tensor to np.array\n",
        "a = torch.ones(5)\n",
        "b = a.numpy()\n",
        "print(a, b)\n",
        "\n",
        "# np.array to torch.tensor\n",
        "c = torch.from_numpy(b)\n",
        "print(b, c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
            "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Geb689ATQA1U"
      },
      "source": [
        "## CUDA Tensors\n",
        "\n",
        "So far, all tensors we used are on the CPU. With the `.to()` method, we can move the tensor to the GPU.\n",
        "\n",
        "Matrix operations on GPUs are much faster than on CPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RydL5B9HP21S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7f7127-f06c-4d17-9f27-138e3aa258b5"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "x  = torch.rand(10)\n",
        "print(x)\n",
        "x = x.to(device)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.6705, 0.7627, 0.9312, 0.1194, 0.9688, 0.9666, 0.3719, 0.4897, 0.9837,\n",
            "        0.1093])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hHgr1-oIuSc"
      },
      "source": [
        "import numpy as np\n",
        "x, y = np.random.rand(8000, 1000), np.random.rand(1000, 8000)\n",
        "x_cuda, y_cuda = torch.from_numpy(x).to('cuda'), torch.from_numpy(y).to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqPTjVTCJQ92"
      },
      "source": [
        "%%time\n",
        "z = np.dot(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qopsC6OJU8P"
      },
      "source": [
        "%%time\n",
        "z = torch.matmul(x_cuda, y_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0L4VIUFJnfO"
      },
      "source": [
        "By oprating on GPUs, we achieved ~1000X speedup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liwMHyYfxqS8"
      },
      "source": [
        "## Automatic Differentiation\n",
        "\n",
        "Automatic differentiation the central functionality of PyTorch. The `autograd` package provides automatic differentiation for all operations on Tensors. \n",
        "\n",
        "``torch.Tensor`` is the central class of the package. If you set its attribute\n",
        "``.requires_grad`` as ``True``, it starts to track all operations on it. When\n",
        "you finish your computation you can call ``.backward()`` and have all the\n",
        "gradients computed automatically. The gradient for this tensor will be\n",
        "accumulated into ``.grad`` attribute.\n",
        "\n",
        "To stop a tensor from tracking history, you can call ``.detach()`` to detach\n",
        "it from the computation history, and to prevent future computation from being\n",
        "tracked.\n",
        "\n",
        "To prevent tracking history (and using memory), you can also wrap the code block\n",
        "in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n",
        "model because the model may have trainable parameters with\n",
        "``requires_grad=True``, but for which we don't need the gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBlh-GTIxpXZ"
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "print(y.grad_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRTCyy6y6MLI"
      },
      "source": [
        "The computation graph looks like below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Sd9jXW56ry"
      },
      "source": [
        "torchviz.make_dot(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZQQuEC96RRN"
      },
      "source": [
        "Now let's try to do more operations on ``y`` and see how the graph would change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqqbQPVc6CxR"
      },
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "torchviz.make_dot(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL7Xde226nBk"
      },
      "source": [
        "As you can see, PyTorch can automatically track our operations on ``torch.tensor``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBnF5c6p63JA"
      },
      "source": [
        "Now, let's take a look at how to compute the gradients.\n",
        "\n",
        "$$o = \\frac{1}{4}\\sum_{i=1}^4 3(x_i + 2)^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MA0-LXJ6gM0"
      },
      "source": [
        "out.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6g7kcku722-"
      },
      "source": [
        "``.backward()`` will automatically detele the stored computation graph. Calling `.backward()` on the same tensor for the second time will cause an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPPRWOfV72HD"
      },
      "source": [
        "out.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNweaiDQ8MHW"
      },
      "source": [
        "You can set `retain_graph=True` when calling `.backward()` so that the graph won't be deleted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WN1W2xU6-2b"
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "out.backward(retain_graph=True)\n",
        "# call the second time\n",
        "out.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS27CDCE8fzb"
      },
      "source": [
        " Notice that now the elements value in `x.grad` has been changed to `9`. This is bacause PyTorch will cumulatively sum up all backward gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUy9uIjT-8HE"
      },
      "source": [
        "You can also stop autograd from tracking history on Tensors\n",
        "with ``.requires_grad=True`` either by wrapping the code block in\n",
        "``with torch.no_grad():``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqdJZmBN-3uo"
      },
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "\tprint((x ** 2).requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b1W__u9-_1B"
      },
      "source": [
        "Or by using ``.detach()`` to get a new Tensor with the same\n",
        "content but that does not require gradients:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpLpvKiJ-9Tq"
      },
      "source": [
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "print(x.eq(y).all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoAetEpydcYF"
      },
      "source": [
        "# Debugging in Colab\n",
        "\n",
        "Most of the time when a Python script fails, it will raise an Exception.\n",
        "When the interpreter hits one of these exceptions, information about the cause of the error can be found in the *traceback*, which can be accessed from within Python.\n",
        "With the ``%xmode`` magic function, IPython allows you to control the amount of information printed when the exception is raised.\n",
        "Consider the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsj9boOIdgOp"
      },
      "source": [
        "def func1(a, b):\n",
        "    return a / b\n",
        "\n",
        "def func2(x):\n",
        "    a = x\n",
        "    b = x - 1\n",
        "    return func1(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3yarGKndy_0"
      },
      "source": [
        "func2(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hx0TkYZd02m"
      },
      "source": [
        "# most compact\n",
        "%xmode Plain\n",
        "func2(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ2DFhIKeJ9F"
      },
      "source": [
        "# default\n",
        "%xmode Context\n",
        "func2(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVMPVC6ZeNui"
      },
      "source": [
        "# gives additional information about function argument\n",
        "%xmode Verbose\n",
        "func2(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSD_Qf0IeiII"
      },
      "source": [
        "## ipdb\n",
        "The standard Python tool for interactive debugging is ``pdb``, the Python debugger.\n",
        "This debugger lets the user step through the code line by line in order to see what might be causing a more difficult error.\n",
        "The IPython-enhanced version of this is ``ipdb``, the IPython debugger.\n",
        "\n",
        "There are many ways to launch and use both these debuggers; we won't cover them fully here.\n",
        "Refer to the online documentation of these two utilities to learn more.\n",
        "\n",
        "In IPython, perhaps the most convenient interface to debugging is the ``%debug`` magic command.\n",
        "If you call it after hitting an exception, it will automatically open an interactive debugging prompt at the point of the exception.\n",
        "The ``ipdb`` prompt lets you explore the current state of the stack, explore the available variables, and even run Python commands!\n",
        "\n",
        "Let's look at the most recent exception, then do some basic tasks–print the values of ``a`` and ``b``, and type ``quit`` to quit the debugging session:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NVSNh5Jet1G"
      },
      "source": [
        "func2(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0gdZZVMp3E-"
      },
      "source": [
        "%debug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbMqjXYdqDzM"
      },
      "source": [
        "**Remember: after you are done with debugging, please remove the extra lines/blocks of code before submsision.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfuMaEiLfCgQ"
      },
      "source": [
        "## Partial list of debugging commands\n",
        "\n",
        "There are many more available commands for interactive debugging than we've listed here; the following table contains a description of some of the more common and useful ones:\n",
        "\n",
        "| Command         |  Description                                                |\n",
        "|-----------------|-------------------------------------------------------------|\n",
        "| ``list``        | Show the current location in the file                       |\n",
        "| ``h(elp)``      | Show a list of commands, or find help on a specific command |\n",
        "| ``q(uit)``      | Quit the debugger and the program                           |\n",
        "| ``c(ontinue)``  | Quit the debugger, continue in the program                  |\n",
        "| ``n(ext)``      | Go to the next step of the program                          |\n",
        "| ``<enter>``     | Repeat the previous command                                 |\n",
        "| ``p(rint)``     | Print variables                                             |\n",
        "| ``s(tep)``      | Step into a subroutine                                      |\n",
        "| ``r(eturn)``    | Return out of a subroutine                                  |\n",
        "\n",
        "For more information, use the ``help`` command in the debugger, or take a look at ``ipdb``'s [online documentation](https://github.com/gotcha/ipdb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzAoAQfq_NWw"
      },
      "source": [
        "# Build and Train a Neural Network \n",
        "\n",
        "`torch.nn` is the package that we are going to use a lot in EECS 442. It provides many useful built-in functions for building neural networks. \n",
        "\n",
        "First, let's take a look at how to define a neural network.\n",
        "\n",
        "For any neural network built in PyTorch, user will need to specific `__init__()` and `forward()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D57x0Vnf_TkA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.out = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        y = self.out(h)\n",
        "        return y\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JemSdMuti86j"
      },
      "source": [
        "We count the number of parameters in the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWmaDMf4i2Rk"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(layer.numel() for layer in model.parameters() if layer.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESJBAe1djXEJ"
      },
      "source": [
        "print(f'Total parameters: {count_parameters(net)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clC9Kngqjuzu"
      },
      "source": [
        "Now, let's try a random input and compute the loss. We will define a mean sqaured error as the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xis_Ljp7lWl0"
      },
      "source": [
        "input = torch.rand(1, 784)\n",
        "output = net(input)\n",
        "target = torch.randn(10)\n",
        "target = target.view(1, -1)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZA7igYjl52j"
      },
      "source": [
        "Great! We have successfully calculated the loss value on a pair of random input-target. Now, let's try to do backpropagation to update the neural network so that it can output approximately the groundtruth target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPuJUrXfkYpm"
      },
      "source": [
        "Remember that `.backward()` would accumulate the gradients. It's a good practice to zero the gradient buffer before we do any backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmIwqYddkUES"
      },
      "source": [
        "net.zero_grad()\n",
        "loss.backward()\n",
        "print(list(net.parameters())[0].grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKVi05_Im1jC"
      },
      "source": [
        "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
        "``weight = weight - learning_rate * gradient``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kz24bWcnzq-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tnrange # progress bar\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg6iSubZm3k-"
      },
      "source": [
        "net = Net()\n",
        "num_iters = 1000\n",
        "learning_rate = 0.01\n",
        "loss_traj = []\n",
        "\n",
        "for _ in tnrange(num_iters):\n",
        "    # step 1: compute loss\n",
        "    output = net(input)\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # step 2: compute gradient\n",
        "    net.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # step 3: update the model parameters\n",
        "    for f in net.parameters():\n",
        "        f.data.sub_(f.grad.data * learning_rate)\n",
        "\n",
        "    # keep track the loss values for visualization\n",
        "    loss_traj.append(loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtDpGbdPk6Yy"
      },
      "source": [
        "plt.plot(np.arange(num_iters), loss_traj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h0rd04JoWsP"
      },
      "source": [
        "Great! We have successfully trained our first neural network to perform perfectly on a random data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVEAVv-6ooaE"
      },
      "source": [
        "# Train a Classifier\n",
        "\n",
        "Train a neural network on a single random data point is not very exciting. Now, let's try to put together what we have learned and train a 2-layer neural network to classify the images in CIFAR-10 dataset.\n",
        "\n",
        "We can easily download the datafrom from `torchvision.datasets`. `torch.utils.data.DataLoader` is useful for creating a data feeding pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYNsMpqto2ym"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05IZfiqNrswv"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[:4]))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv1tDZOgsBUE"
      },
      "source": [
        "Previously, we trained a simple multilayer perceptron. Here, we will define and train a convolutional neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTPw0sG_sAdb"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32 * 3)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# initialize the neural netowrk and send it to the GPU\n",
        "net = Net().cuda()\n",
        "summary(net, input_size = (3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1I77Od_stA5"
      },
      "source": [
        "Now, we have successfully defined a neural network with 2 convolutional layers and 3 linear layers. Let's define the loss, the optimizer, and train the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEJ148vIr2CR"
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = 5e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in tnrange(num_epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    print(f'Epoch {epoch} loss: {running_loss}')\n",
        "    running_loss = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRe72GckLv1W"
      },
      "source": [
        "Save the trained model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fxrMs6Etq_g"
      },
      "source": [
        "# save the trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# load the saved model\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca3IqsVKu-1I"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "pred_list = []\n",
        "labels_list = []\n",
        "# no need to keep track of the computation graph when testing\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        labels_list.append(labels.detach().numpy())\n",
        "        pred_list.append(predicted.detach().numpy())\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKjeD5Xxvg99"
      },
      "source": [
        "Now, let's plot the confusion matrix to understand how the model performs on each of the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWxQaRsvv2tA"
      },
      "source": [
        "labels = np.concatenate(labels_list)\n",
        "preds = np.concatenate(pred_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gfiVLYivVVf"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "conf_matrix = confusion_matrix(preds, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtdeMCn8we0X"
      },
      "source": [
        "sns.heatmap(conf_matrix, cmap=\"YlGnBu\", yticklabels=classes, xticklabels=classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JspthlXSwIwt"
      },
      "source": [
        "The network tends to misclassify cat as dog, truck as car, deer as bird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Srt97Okfc8y"
      },
      "source": [
        "# Excercise (Optional)\n",
        "\n",
        "Now you have a good understanding of the basics of PyTorch. Now it's your turn to practice your new skills.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2TPaZ3mjmRO"
      },
      "source": [
        "## Basic Operations\n",
        "\n",
        "Use [`torch.bmm`](https://pytorch.org/docs/1.1.0/torch.html#torch.bmm) to perform a batched matrix multiply.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCkzMT22jrbV"
      },
      "source": [
        "B, N, M, P = 3, 2, 5, 4\n",
        "x = torch.rand(B, N, M)  # Random tensor of shape (B, N, M)\n",
        "y = torch.rand(B, M, P)  # Random tensor of shape (B, M, P)\n",
        "\n",
        "# We can use a for loop to (inefficiently) compute a batch of matrix multiply\n",
        "# operations\n",
        "z1 = torch.empty(B, N, P)  # Empty tensor of shape (B, N, P)\n",
        "for i in range(B):\n",
        "  z1[i] = x[i].mm(y[i])\n",
        "print('Here is the result of batched matrix multiply with a loop:')\n",
        "print(z1)\n",
        "\n",
        "z2 = None\n",
        "##############################################################################\n",
        "# TODO: Use bmm to compute a batched matrix multiply between x and y; store  #\n",
        "# the result in z2.                                                          #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "pass\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('\\nHere is the result of batched matrix multiply with bmm:')\n",
        "print(z2)\n",
        "\n",
        "# The two may not return exactly the same result; different linear algebra\n",
        "# routines often return slightly different results due to the fact that\n",
        "# floating-point math is non-exact and non-associative.\n",
        "diff = (z1 - z2).abs().max().item()\n",
        "print('\\nDifference:', diff)\n",
        "print('Difference within threshold:', diff < 1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9BUOhaojo_o"
      },
      "source": [
        "## Tensor Indexing\n",
        "\n",
        "Practice with boolean masks by implementing the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj_-6FSpmaH9"
      },
      "source": [
        "def num_negative(x):\n",
        "  \"\"\"\n",
        "  Return the number of negative values in the tensor x\n",
        " \n",
        "  Inputs:\n",
        "  - x: A tensor of any shape\n",
        " \n",
        "  Returns:\n",
        "  - num_neg: Number of negative values in x\n",
        "  \"\"\"\n",
        "  num_neg = 0\n",
        "  ##############################################################################\n",
        "  # TODO: Use boolean masks to count the number of negative elements in x.     #\n",
        "  ##############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  pass\n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return num_neg\n",
        "\n",
        "# Make a few test cases\n",
        "torch.manual_seed(598)\n",
        "x0 = torch.tensor([[-1, -1, 0], [0, 1, 2], [3, 4, 5]])\n",
        "x1 = torch.tensor([0, 1, 2, 3])\n",
        "x2 = torch.randn(100, 100)\n",
        "assert num_negative(x0) == 2\n",
        "assert num_negative(x1) == 0\n",
        "assert num_negative(x2) == 4984\n",
        "print('num_negative seems to be correct!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EQjX3_Xl_Cs"
      },
      "source": [
        "##  Broadcasting\n",
        "\n",
        "Write a function that normalizes the columns of a matrix. It should compute the mean and standard deviation of each column, then subtract the mean and divide by the standard deviation for each element in the column.\n",
        "\n",
        "Example:\n",
        "```\n",
        "x = [[ 0,  30,  600],\n",
        "     [ 1,  10,  200],\n",
        "     [-1,  20,  400]]\n",
        "```\n",
        "- The first column has mean 0 and std 1\n",
        "- The second column has mean 20 and std 10\n",
        "- The third column has mean 400 and std 200\n",
        "\n",
        "After normalizing the columns, the result should be:\n",
        "```\n",
        "y = [[ 0,  1,  1],\n",
        "     [ 1, -1, -1],\n",
        "     [-1,  0,  0]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7se1khglmBY_"
      },
      "source": [
        "def normalize_columns(x):\n",
        "  \"\"\"\n",
        "  Normalize the columns of a matrix by subtracting the mean and dividing by the\n",
        "  standard deviation.\n",
        " \n",
        "  Inputs:\n",
        "  - x: Tensor of shape (N, M)\n",
        "  \n",
        "  Returns:\n",
        "  - y: Tensor of shape (N, M) which is a copy of x with normalized columns.\n",
        "  \"\"\"\n",
        "  y = x.clone()\n",
        "  ##############################################################################\n",
        "  # TODO: Complete the implementation of this function. Do not modify x.       #\n",
        "  # Your implementation should not use any loops; instead you should use       #\n",
        "  # reduction and broadcasting operations.                                     #\n",
        "  ##############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  pass\n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adfFPzl9mXfX"
      },
      "source": [
        "x0 = torch.tensor([[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]])\n",
        "y0 = normalize_columns(x0)\n",
        "print('Here is x0:')\n",
        "print(x0)\n",
        "print('Here is y0:')\n",
        "print(y0)\n",
        "assert y0.tolist() == [[0., 1., 1.], [1., -1., -1.], [-1., 0., 0.]]\n",
        "assert x0.tolist() == [[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}